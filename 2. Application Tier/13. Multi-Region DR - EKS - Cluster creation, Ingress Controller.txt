Provision an Amazon Linux EC2 instance.

# Install kubectl

curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.27.1/2023-04-19/bin/linux/amd64/kubectl
chmod +x ./kubectl
mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH
kubectl version --short --client (for verification)

# Install eksctl

ARCH=amd64
PLATFORM=$(uname -s)_$ARCH
curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
sudo mv /tmp/eksctl /usr/local/bin 
eksctl version (for verification)

# In AWS console, VPC -> remove any previous EKS cluster VPC that might not have been deleted properly

# create cluster
eksctl create cluster --name=eksdemo1 \
                      --region=us-east-2 \
                      --zones=us-east-2a,us-east-2b \
                      --without-nodegroup 

Note that this command will create a new VPC in the specified region with 1 public subnet and 1 private subnet in each of the 2 zones.
The cluster will be created in this VPC.

# List clusters in a region
eksctl get clusters --region=us-east-2

# To enable and use AWS IAM roles for Kubernetes service accounts on our EKS cluster, we must create & associate OIDC identity provider.
eksctl utils associate-iam-oidc-provider \
    --region us-east-2 \
    --cluster eksdemo1 \
    --approve

# Create Private Node Group   
eksctl create nodegroup --cluster=eksdemo1 \
                       --region=us-east-2 \
                       --name=eksdemo1-ng-private1 \
                       --node-type=t3.medium \
                       --nodes=2 \
                       --nodes-min=2 \
                       --nodes-max=4 \
                       --node-volume-size=20 \
                       --ssh-access \
                       --ssh-public-key=MyEast2KeyPair \
                       --managed \
                       --asg-access \
                       --external-dns-access \
                       --full-ecr-access \
                       --appmesh-access \
                       --alb-ingress-access \
                       --node-private-networking

# Download IAM Policy
# Needed only if AWSLoadBalancerControllerIAMPolicy is not created yet
curl -o iam_policy_latest.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json

# Download specific version if needed
# Needed only if AWSLoadBalancerControllerIAMPolicy is not created yet
curl -o iam_policy_v2.3.1.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.3.1/docs/install/iam_policy.json

# Create IAM Policy using policy downloaded 
# Needed only if AWSLoadBalancerControllerIAMPolicy is not created yet
aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --policy-document file://iam_policy_latest.json

# Go to AWS Console and verify the policy and its permissions
# Make a note of the policy ARN: arn:aws:iam::XXXXXXXXXXXX:policy/AWSLoadBalancerControllerIAMPolicy

# Create iam service account in kubernetes cluster using the above policy
eksctl create iamserviceaccount \
  --cluster=eksdemo1 \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --region=us-east-2 \
  --attach-policy-arn=arn:aws:iam::XXXXXXXXXXXX:policy/AWSLoadBalancerControllerIAMPolicy \
  --override-existing-serviceaccounts \
  --approve

# Verify using eksctl and kubectl
eksctl  get iamserviceaccount --cluster eksdemo1 --region=us-east-2
kubectl get sa -n kube-system
kubectl get sa aws-load-balancer-controller -n kube-system
kubectl describe sa aws-load-balancer-controller -n kube-system
You can see that newly created policy ARN is added in Annotations confirming that AWS IAM policy with required ALB related access is bound to a Kubernetes service account

# Install helm needed to install the load balancer controller
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 > get_helm.sh
chmod 700 get_helm.sh
./get_helm.sh

# Verify helm
helm version --short | cut -d + -f 1

# Add the eks-charts repository.
helm repo add eks https://aws.github.io/eks-charts

# Update your local repo to make sure that you have the most recent charts.
helm repo update

# Install the AWS Load Balancer Controller in the kubernetes cluster using the iam serviceaccount created earlier
# ENSURE TO USE THE VPC ID OF THE EKS CLUSTER here

helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=eksdemo1 \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=us-east-2 \
  --set vpcId=vpc-0258f7639972f4384 \
  --set image.repository=public.ecr.aws/eks/aws-load-balancer-controller

# If needed to uninstall
helm uninstall aws-load-balancer-controller -n kube-system

# Verify that the controller & webhook service are installed.
kubectl get pods -n kube-system
kubectl get deploy -n kube-system
kubectl get svc -n kube-system
Note: Traffic coming to "aws-load-balancer-webhook-service" on port 443 will be sent to port 9443 on "aws-load-balancer-controller" deployment related pods. 

# Create RDS DB in private subnets

If, for local testing, you had created a RDS DB in public subnets, drop the DB. Now, we need the DB in private subnets.

Pre-requisite-1: Create DB Security Group:

Create security group to allow access for RDS Database on port 3306 in the VPC in which the EKS cluster is created.

Security group name: eks_rds_db_sg
Description: Allow access for RDS Database on Port 3306
VPC: eksctl-eksdemo1-cluster/VPC
Inbound Rules
Type: MySQL/Aurora
Protocol: TPC
Port: 3306
Source: Anywhere (0.0.0.0/0)

Note: Anywhere is not needed. The nodes created in the nodegroup will have 2 security groups associated with them - one with pattern "remoteAccess" and another without this pattern.
The security group without the pattern "remoteAccess" is what actually needs to be used as the Source in eks_rds_db_sg to ensure strictest possible security. Because the RDS is in private subnet, you anyway can't access from outside VPC (through my desktop MySQL workbench for example), but limiting to the particular security group is best practice. I tried and it works !

Description: Allow access for RDS Database on Port 3306
Outbound Rules
Leave to defaults

Pre-requisite-2: Create DB Subnet Group in RDS

As said earlier, the eks create cluster command would have created a VPC and 1 Private subnet, 1 public subnet in each of the 2 AZs in which you chose to create the cluster.
Note down the Private Subnets in the VPC in which your EKS cluster was created.
Go to RDS -> Subnet Groups
Click on Create DB Subnet Group
Name: eks-rds-db-subnetgroup
Description: EKS RDS DB Subnet Group
VPC: eksctl-eksdemo1-cluster/VPC
Availability Zones: us-east-2a, us-east-2b
Subnets: Select the Private subnets from both your AZs
Click on Create

Instead of:
Create RDS Database
In the Connectivity section, make sure you create in the VPC of the EKS cluster and the subnet group (with private subnets) created above
VPC: eksctl-eksdemo1-cluster/VPC
Subnet Group: eks-rds-db-subnetgroup
Publicly accessible: No

From "my-machine" Amazon Linux Ec2 instance, connect to RDS DB and check if you are able to read the table created in the primary region us-east-1.
Note the usage of the new region RDS DNS endpoint when connecting to the mysql shell.

kubectl run mysql-client --image=mysql:8.0 -it --rm --restart=Never -- /bin/bash
mysql -h database-1.c7wbirg5zeed.us-east-2.rds.amazonaws.com -uadmin -pTest1234

mysql> show schemas;
mysql> use ml_db;
mysql> select * from diamond_price_app;


============================

DR part:

Do this in RS section:
Go to us-east-1 -> RDS -> Create Read Replica into us-east-2 region in the subnet created above and using the security group created above

Modify app-related -> handler_diamond_price.py to point to Read Replica (RR) RDS instance.
Upload Dockerfile, py file and pkl into s3://vinod-ml-sagemaker-bucket/forDR
Go to the EC2 connection OUTSIDE the container i.e. on the VM
aws s3 cp s3://vinod-ml-sagemaker-bucket/forDR/handler_diamond_price.py . 
aws s3 cp s3://vinod-ml-sagemaker-bucket/forDR/diamond_price_dt_model.pkl . 
aws s3 cp s3://vinod-ml-sagemaker-bucket/forDR/Dockerfile . 

-- install docker

sudo yum install docker
sudo systemctl start docker

-- build image using docker and the py/pkl/Dockerfile.

sudo docker build -t diamond_price_predictor_app .

-- push image to repo

sudo docker login (provide username and password)
sudo docker tag diamond_price_predictor_app vinods03/diamond_price_predictor_app:v8 
sudo docker push vinods03/diamond_price_predictor_app:v8

kube-manifests related changes:

Update the image version for diamond-price-app in the kube-manifests folder to v8.
Provision a new certificate in us-east-2 for *.tekedify.net and update the ingress resources in the kube-manifests folder to point to this new certificate.
Upload the latest kube-manifests in s3://vinod-ml-sagemaker-bucket/forDR/kube-manifests

# kube-manifests
mkdir kube-manifests
cd kube-manifests
aws s3 cp s3://vinod-ml-sagemaker-bucket/forDR/kube-manifests/ . --recursive
cd ..
kubectl config set-cluster eksdemo1 
kubectl apply -R -f kube-manifests (-R because of the sub-folder structure)

Check all app-related resources are created:

kubectl get svc -n dev
kubectl get pods -n dev
kubectl get deploy -n dev
kubectl get ingress -n dev -> 3 different ingress resources must be listed but all have the same ALB DNS address.

Check in AWS Console, if load balancer and Target group are created.
Verify: In AWS Console -> Load Balancer -> Listeners and Rules -> verify that, for https:443, certificate is associated.
We need the application load balancer DNS for the verification of our app.

Note that, in the "Listeners and rules" section of the load balancer, there should be 2 protocols / ports listed now (http:80 and https:443), thanks to the ssl annotations in ingress-resource.yml. Before the addition of the 'redirect' annotation, each of the 2 protocols / ports will be associated with the 3 rules defined in ingress-resource.yml. After the addition of the 'redirect' annotation, there will still be 2 protocols / ports listed but the http:80 protocol / port will have only 1 rule related to redirection to https:443 and https:443 alone will be associated with the 3 rules defined in ingress-resource.yml.

Each of the 3 rules will be associated with a separate target group.
Each of the 3 target groups will be associated with the instances in the private subnet of our EKS cluster.
The 3 rules, defined in the ingress-resource yaml of our kube-manifests are:
Path Pattern is /diamond_price_predict OR /diamond_price_predict/* OR
Path Pattern is /car_price_predict OR /car_price_predict/* OR
The default rule
So the 3 replicas of each service/application will run in the 2 private subnet instances of our EKS cluster.

Also, verify that all target groups are healthy.
Corresponding to the 3 rules, 3 target groups will be created. 
Each target group has the 2 EKS worker nodes as the targets. 
Each of our services will be running on all the worker nodes of the EKS cluster. 

Now, in Route 53, create another A record for Secondary Failover pointing to the us-east-2 ALB.

=========================================================

Now, lets try DB DR:
Edit the Security Group of the primary DB instance in us-east-1 and remove the rule that allows MySQL traffic on port 3306 from anywhere.

Now, from EC2 instance if you try:

import requests

url = 'https://mlapps.tekedify.net/diamond_price_predict'

r = requests.post(url, json = {
    "carat": 0.31,
    "cut": "Good",
    "color": "J",
    "clarity": "SI2",
    "depth": 63.3,
    "table": 58.0,
    "x": 4.34,
    "y": 4.35,
    "z": 2.75
})

print(r)

print(r.text)

you will get:

>>> print(r)
<Response [500]>
>>> 
>>> print(r.text)
<!doctype html>
<html lang=en>
<title>500 Internal Server Error</title>
<h1>Internal Server Error</h1>
<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>

Remember read replica doesnt allow write till it is promoted to stand-alone instance.

--

This will work though as the car app, at present does not load into MySQL DB.

import requests

url = 'https://mlapps.tekedify.net/car_price_predict'

r = requests.post(url, json = {
    "symboling": 2,
    "normalized-losses": 164,
    "wheel-base": 99.8,
    "make": "audi",
    "fuel-type": "gas",
    "aspiration": "std",
    "num-of-doors": "four",
    "body-style": "sedan",
    "drive-wheels": "fwd",
    "engine-location": "front",
    "length": 176.60,
    "width": 66.20,
    "height": 54.30,
    "curb-weight": 2337,
    "engine-type": "ohc",
    "num-of-cylinders": "four",
    "engine-size": 109, 
    "fuel-system": "mpfi",
    "bore": 3.19,
    "stroke": 3.40,
    "compression-ratio": 10,
    "horsepower": 102,
    "peak-rpm": 5500,
    "city-mpg": 24,
    "highway-mpg": 30
})

print(r)

print(r.text)

works as expected.

----------------

Promote the RDS Read Replica.

Try this again:

python3

import requests

url = 'https://mlapps.tekedify.net/diamond_price_predict'

r = requests.post(url, json = {
    "carat": 0.31,
    "cut": "Good",
    "color": "J",
    "clarity": "SI2",
    "depth": 63.3,
    "table": 58.0,
    "x": 4.34,
    "y": 4.35,
    "z": 2.75
})

print(r)

print(r.text)

This should go through:

>>> print(r)
<Response [200]>
>>> 
>>> print(r.text)
The run id is 1716704100 and the dimanond price is 335.0

-------------------------- SUMMARIZING --------------------------------------

So while cross-AZ is seamless, cross-Region is not that easy.
You need to have another version of the app that is pointing to the RDS Read Replica endpoint. This endpoint is different from the main instance endpoint.
You need to have another version of kube-manifests pointing to a different version of the app that is pointing to the RDS Read Replica endpoint.
You need to have another version of kube-manifests pointing to a different certificate for the ingress resources in the different region.
You need to have Primary & Secondary Failover A records in Route 53 - each pointing to the ALB in the 2 different regions.

If primary DB alone goes down, you need to manually promote the Read Replica.
The API Gateway, Queues, Lambda can still be from Primary region.
The lambda will invoke mlapps.tekedify.net -> primary ALB -> Target groups pointing to apps inserting into Primary DB fail -> fail over to secondary ALB -> Target groups pointing to apps inserting into Secondary DB that has now been promoted succeed

If entire primary region goes down,  you need to manually promote the Read Replica.
The API Gateway, Queues, Lambda also need to be available in the Secondary region.
Through Route 53, the front-end should automatically be routed to the secondary region.

In either case, better to create another Read Replica out of this standalone RDS instance, update the app / kube-manifests and prepare for the next DR.

----------------------------------------------------------------------------------

# Cleanup (Only after you have completed and validated the remaining steps detailed out in subsequent notes)

Delete database

Delete DB security group

Delete DB subnet group

kubectl delete -R -f kube-manifests

========

# delete Node Group
eksctl delete nodegroup --cluster=eksdemo1 --name=eksdemo1-ng-private1 --region us-east-2

# Delete Cluster
eksctl delete cluster eksdemo1 --region us-east-2

or 

go to CloudFormation: delete service account stack, nodegroup stack and finally eks cluster stack.

========

Delete my-machine EC2 instance


